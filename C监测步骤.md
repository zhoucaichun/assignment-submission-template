## 研究内容
1. 对比分析 Giraph 和 MapReduce 在执行 PageRank 算法等图迭代计算任务时的差异。
2. 深入理解 Giraph 所采用的 BSP (Bulk Synchronous Parallel) 设计理念及其在图计算中的优势。
3. 重点探讨两者在数据通信方式、任务调度机制及迭代开销等方面的不同，以及这些差异对算法性能与可扩展性的影响。
## 实验设计
1. 分别基于Giraph和MapReduce实现PageRank算法，从编程模型角度分析两种系统在算法表达与数据流处理方式上的差异。
2. 在不同规模的图数据集上运行实验，记录作业执行时间、网络通信量以及内存占用等指标，对比分析Giraph与MapReduce在图迭代计算中的性能表现。

## 研究内容与实验设计里的一些任务解释：
#### 1. 算法性能 (Algorithm Performance)
不仅仅是“跑得快不快”，它包含三个维度：
* **执行效率 (Execution Efficiency)**：完成相同任务（如 PageRank 收敛到相同精度）所需的**总时间 (Job Completion Time, JCT)**。这是最直观的指标。
* **资源效率 (Resource Efficiency)**：为了达到上述效果，消耗了多少 CPU、内存和磁盘 I/O。如果 Giraph 比 MR 快 5 倍，但内存消耗大了 10 倍，这就是性能的权衡（Trade-off）。
* **收敛速度 (Convergence Speed)**：达到数值稳定需要迭代多少轮。
#### 2. 可扩展性 (Scalability)
在分布式计算中，它衡量系统“抗压能力”的强弱：
* **数据可扩展性 (Data Scalability)**：当图数据从小（随机图）变大（Web图），或者从稀疏变稠密时，系统性能是否线性下降？（还是会直接崩溃？）
* **并发可扩展性 (Concurrency Scalability)**：**这是你引入调度器实验的核心意义。** 当多个作业同时提交时，系统能否维持稳定的性能？（FIFO 可能导致后续任务“饿死”，可扩展性差；Fair/Capacity 能维持多任务并行，可扩展性好）。
#### 3. 任务调度机制及迭代开销等方面的不同——用Straggler (长尾任务) （`analyze_stragglers.py` 获取的）
在分布式计算中，一个作业（Job）会被拆分成成千上万个任务（Task）。**Straggler 指的是那些运行时间远远超过平均水平的“慢任务”。
* **1、成因**：可能是数据倾斜（某个节点处理的图顶点特别多）、机器性能波动（坏盘、CPU降频）或调度分配不均。
* **2、形象比喻**：木桶效应：**一个 BSP 作业跑得有多快，取决于最慢的那个 Task。**
* **3（1）、对于 MapReduce**：Straggler 的影响是**局部**的。其他 Task 跑完了可以先休息，或者启动推测执行（Speculative Execution）。
* **3（2）、对于 Giraph (BSP 模型)**：Straggler 的影响是**全局且致命**的。因为 BSP 有“同步屏障 (Synchronization Barrier)”，**只要有一个 Task 没跑完，全集群几百个节点都必须停下来等着，不能进入下一轮迭代。** 这就是巨大的“迭代开销”。
* **4、`analyze_stragglers.py` 的监控目的**——量化“木桶效应”的严重程度
    * 如果脚本显示 Straggler 数量少，但等待时间极长 -> 证明 BSP 模型的同步开销大。
    * 如果脚本显示 Fair 调度器下的 Straggler 比 FIFO 少 -> 证明公平调度能缓解长尾现象，提升性能。 

#### 4. 算法表达 (Algorithm Expression)
**意思**：程序员如何用代码去“描述”这个算法？
* **MapReduce (以数据为中心)**：你被迫把 PageRank 拆解成 **Map**（分发权重）和 **Reduce**（汇总权重）两个阶段。
    * *痛点*：逻辑是割裂的。每一次迭代，都要把图结构重新读一遍，算出数值，再写回去。你关注的是“键值对 (Key-Value Pairs)”怎么变。
* **Giraph (以顶点为中心, Vertex-Centric)**：你像“图中的一个节点”一样思考。
    * *优势*：你只需要写一个 `compute()` 函数。节点一直活着（常驻内存），收到邻居的消息，更新自己，发消息给邻居。你关注的是“顶点 (Vertex)”的行为。

#### 5. 数据流处理方式 (Data Flow Processing)
**意思**：数据在计算过程中是怎么流动的？（这是性能差异的根本原因）
* **MapReduce (磁盘流)**：
    * `HDFS (读)` -> `Map` -> `Disk (溢写)` -> `Network (Shuffle)` -> `Reduce` -> `HDFS (写)`
    * **核心特征**：**“无状态”**。这一轮迭代做完，所有数据必须落地回 HDFS，下一轮迭代重新从 HDFS 读。**数据流主要是 磁盘 -> 内存 -> 磁盘。**
* **Giraph (内存/网络流)**：
    * `HDFS (一次性加载)` -> `Memory` -> `Network (消息)` -> `Memory` -> ... -> `HDFS (最后输出)`
    * **核心特征**：**“有状态”**。图顶点一直待在内存里。**数据流主要是 内存 -> 网络 -> 内存。**

---


# C 终极执行手册：

**你的核心职责**：通过实验数据回答两个核心问题：
1.  **框架之争**：为什么 Giraph 的 BSP 模型在图计算上比 MapReduce 更优？（对应研究内容 1, 2, 3）
2.  **调度之争**：不同的 YARN 调度器如何影响任务的排队与资源分配？
---

## 🛠️ 第一阶段：准备监控武器 (Preparation)

在开始跑任务前，必须在 **Slave 节点（如 ecnu02）** 准备好采集工具。

1.  **安装系统级监控 (`dstat`)**
    * **作用**：抓取 CPU、内存、磁盘 I/O、网络 I/O 的实时波动曲线。
    * **命令**：确保 `ecnu02` 上能运行 `dstat`。
```
ps：三个子节点都已安装好了
```
2.  **准备长尾分析脚本 (`analyze_stragglers.py`)**
    * **作用**：分析 Task 粒度的等待时间和执行时间，这是 File 1 中分析“调度公平性”的核心。

---

## ⚔️ 第二阶段：实验执行矩阵 (The Matrix)

你需要完成 **3 轮大实验**（对应 3 种调度器），每一轮都要跑 **2 种框架**和**3个数据集**。

### 实验流程

**对于每一种调度器（FIFO -> Capacity-> Fair ）：**

1.  **修改配置 & 重启 YARN** ( 01节点操作 )。
2.  **实验 A：运行 MapReduce PageRank**
    * 启动 `dstat` 监控 -> 提交作业 -> 结束监控 -> 保存 CSV。
    * **立即记录 UI 详细数据**（参考第三阶段）。
3.  **实验 B：运行 Giraph PageRank**
    * 同上流程。
4.  **实验 C（仅 FIFO 模式）：高分并发测试**
    * 先提交一个大的 Giraph 任务，10秒后马上提交一个 MapReduce 任务。
    * **截图**：YARN 界面上 MR 任务一直处于 `ACCEPTED`（等待）状态的画面。

---


## 📊 第三阶段：指标采集 & 分析

**填表**。请将采集到的数据填入 Excel，并参考下方的模板撰写初步分析。

### 1. 宏观效率指标 (Time & Efficiency)
> **核心研究点**：量化 BSP 模型相比 MapReduce 的性能提升倍数。

| 指标名称 | 获取方式 | 你的动作 | **监测目的与论文意义 (为什么测这个？)** |
| :--- | :--- | :--- | :--- |
| **作业总耗时 (JCT)** | UI: `Elapsed` | 记录秒数 | **性能基准**：直接量化 Giraph 比 MR 快多少倍。通常 Giraph 因内存计算优势，耗时应远小于 MR。 |
| **作业等待时间** | UI: `Start`-`Submit` | 计算差值 | **并发可扩展性**：在 FIFO 模式下，此数值大说明系统并发能力差；在 Fair 模式下数值小，说明系统扩展性好，支持多用户。 |
| **平均 Task 时间** | UI: Task List | 算平均值 | **计算密度**：分析单个分片（Split/Partition）的处理速度，反映框架的基础计算效率。 |
 **作业吞吐量** | 计算得出 | `总边数 / 总耗时` | **数据可扩展性 (Data Scalability)**：在 Small, Medium, Large 三组数据下，观察吞吐量是线性增长还是因 I/O 瓶颈而下降。这是评估系统处理大规模图数据能力的核心指标。 |

**📝分析：**

* **【框架对比：Giraph vs MR】**
    * 在相同数据集下，Giraph 的总耗时显著低于 MapReduce。这是因为 Giraph 采用了 **BSP (Bulk Synchronous Parallel)** 模型，所有计算在内存中常驻进行，避免了 MapReduce 在每次迭代之间必须进行的 **Shuffle 和 HDFS 读写** 开销。
    * MR 需要启动多个 Job 级联来完成 PageRank 迭代，每次启动都有巨大的 **JVM 启动开销** 和调度延迟，而 Giraph 仅需启动一次作业。

* **【调度对比：FIFO vs Fair/Capacity】**
    * 在单任务运行时，三种调度器差异不大。但在并发测试中（结合实验 C），**FIFO** 导致后续任务（无论 MR 还是 Giraph）必须等待前序大作业完全结束，等待时间极长；而 **Fair/Capacity** 允许小任务插队或并行运行，显著降低了平均等待时间。

---
---

### 2. 算法表达与模型开销 (Algorithm Expression)
> **核心研究点**：从编程模型（Model）角度，分析 MapReduce “无状态”特性带来的系统级开销。

| 指标名称 | 获取方式 | 你的动作 | **监测目的与论文意义 (为什么测这个？)** |
| :--- | :--- | :--- | :--- |
| **作业/迭代启动次数**<br>(Job Count) | **UI**: `Job History` | 数 Job ID 个数 | **编程模型差异铁证**：<br>1. **MR**: 迭代 N 轮 = **N 个 Job**。证明 MR 模型无法保持上下文，必须通过“作业级联”实现迭代。<br>2. **Giraph**: 无论迭代多少轮，只有 **1 个 Job**。证明其是“有状态”的顶点中心模型。 |
| **系统启动开销**<br>(Setup/Cleanup Time) | **UI**: `Job Overview` | 估算每次迭代的非计算时间 | **量化模型损耗 (Overhead)**：MapReduce 每次迭代都要重新申请资源、启动 JVM（约 10-30秒）。在小数据集上，这个**“模型税”**甚至超过了计算时间本身，直接导致效率低下。 |

**📝 分析：**

* **【算法表达差异】**
    * **MapReduce 的“无状态”困境**：由于 MR 的 Map 和 Reduce 任务在结束时销毁，无法保留上一轮的图状态。因此，PageRank 算法被迫拆解为 **[填入 Job 数量]** 个独立的作业。
    * **Giraph 的“有状态”优势**：Giraph 采用以顶点为中心（Vertex-Centric）的编程模型，Worker 进程常驻内存，上下文（Context）在整个迭代周期内保持，消除了 **[填入 Setup 总时间]** 秒的重复启动开销。

---

### 3. 核心架构与 IO 指标 (Architecture & I/O)
> **核心研究点**：通过 I/O 数据证明 MapReduce 的“磁盘流”与 Giraph 的“内存流”差异。

| 指标名称 | 英文对应 | 数据源 | 你的动作 | **监测目的与论文意义 (为什么测这个？)** |
| :--- | :--- | :--- | :--- | :--- |
| **HDFS 写入总量** | `Bytes Written` | **[Hadoop UI]** | **关键证据**：对比两者的数量级差异。 | **验证架构差异**：量化 MapReduce 因“无状态”特性导致的 I/O 惩罚。MR 每次迭代必须将状态落地到 HDFS，而 Giraph 保持在内存。这是两者**执行时间差距的根本来源**。 |
| **HDFS 读取总量**<br>(Bytes Read) | **UI**: `Counters` -> `File System` | 记录数值 | **迭代机制差异**：MR 每次迭代都要**重新读取**全量图结构数据，I/O 压力随迭代次数线性增加；而 Giraph 只在作业开始时读取**一次**。 |
| **磁盘 I/O 波动** | `dsk/total` | **[dstat CSV]** | 观察是否有持续的读写波峰。 | **定位性能瓶颈**：通过波形差异（MR 通常呈现周期性的锯齿状高 I/O，Giraph 则较平稳）证明 MR 的计算受限于**磁盘带宽**，而 Giraph 成功规避了这一瓶颈。 |
| **内存峰值** | `Physical Memory` | **[Hadoop UI]** | 确认 Giraph 是否占满了容器内存。 | **分析资源边界 (Trade-off)**：论证 Giraph 的高性能是建立在**高内存消耗**基础上的（空间换时间）。如果此数值接近上限，说明 Giraph 处理更大规模图数据时面临**可扩展性挑战**。 |
| **Spilled Records** | `Spilled Records` | **[Hadoop UI]** | MR 若此数值高，说明发生溢写，性能大降。 | **揭示 MR 机制短板**：这是 MapReduce 并不适合图计算的**微观证据**。大量的 Spill 意味着 MR 在 Shuffle 阶段花费了大量时间进行没必要的**排序和磁盘落盘**，导致迭代效率极低。 |

**📝 分析：**

* **【框架对比：Giraph vs MR】**
    * **磁盘 I/O**：MapReduce 的 HDFS 写入量是 Giraph 的数倍甚至数十倍。这是因为 MR 必须将每轮 PageRank 的中间结果（PR值）写入磁盘以传递给下一轮 Map，而 Giraph 将图顶点保存在 **内存** 中，仅在最后一步输出结果。
    * **dstat 证据**：在 dstat 图表中，MR 呈现出剧烈的、周期性的磁盘 I/O 震荡（读->算->写），而 Giraph 的磁盘 I/O 曲线相对平滑，仅在加载图和输出时有波动。
* **【数据流处理方式】**
    * 实验数据表明，MapReduce 呈现出 **“磁盘流 (Disk-based Data Flow)”** 特征，其 HDFS 读取与写入量随迭代轮数线性增加（MR 产生了 **[填数值] GB** 的写操作）；而 Giraph 呈现出 **“内存流 (Memory-based Data Flow)”** 特征，HDFS 写入量极小（仅 **[填数值] MB**），中间结果完全在内存中流转。
* **【调度对比】**
    * 当资源紧张时（Capacity 限制队列资源），MapReduce 由于频繁读写磁盘，更容易受到 **I/O 争用** 的影响。而合理配置的 Fair 调度器能更好地隔离不同类型的负载，避免 I/O 密集的 MR 任务饿死其他计算任务。

---

### 4. 通信与网络指标 (Communication & Synchronization)
> **核心研究点**：验证 BSP 的“超步（Superstep）”同步机制。

| 指标名称 | 英文对应 | 数据源 | 你的动作 | **监测目的与论文意义 (为什么测这个？)** |
| :--- | :--- | :--- | :--- | :--- |
| **网络流量波形** | `net/total` | **[dstat CSV]** | 观察波形特征（是持续高还是脉冲式）。 | **验证计算模型差异 (BSP vs MapReduce)**：Giraph 基于 BSP 模型，其网络流量应呈明显的**“脉冲式” (Pulse)**，波峰即为 Superstep 间的**同步屏障 (Synchronization Barrier)**。分析脉冲间隔可以评估同步开销；而 MR 的 Shuffle 阶段通常表现为持续的高吞吐，缺乏这种规律的同步特征。 |
| **Shuffle 传输量** | `Reduce shuffle bytes` | **[Hadoop UI]** | 仅 MR 有此指标，重点记录。 | **量化通信开销**：Shuffle 是 MapReduce 的性能杀手。对于 PageRank，MR 每次迭代都要通过网络传输大量的中间结果（图结构+Rank值）。此数据量巨大，直接证明了 MR 在**数据局部性 (Data Locality)** 上的劣势，解释了为何其网络压力远高于 Giraph。 |
| **Superstep 次数** | Log / Counters | **[Hadoop UI]** | 记录 Giraph 的迭代次数。 | **算法收敛性分析**：作为“逻辑时钟”。对比达到相同 PageRank 精度时，Giraph 需要的 Superstep 数量与 MR 需要的 Job 数量。这在论文中用于建立**时间/迭代次数**的基准，证明 Giraph 单次迭代极快。 |
| **Spilled Records** | `Spilled Records` | **[Hadoop UI]** | 检查是否为0（特别是 Giraph）。 | **探测性能拐点**：**关键的可扩展性指标**。对于 Giraph，此数值应为 0。一旦非 0，说明内存不足导致数据溢写到磁盘，性能将**断崖式下跌**。这在论文中用于讨论 Giraph “纯内存计算”模式的物理局限性（即图不能无限大）。 |

**📝 分析：**

* **【框架对比：Giraph vs MR】**
    * **通信模式**：MapReduce 的通信主要集中在 Shuffle 阶段，网络流量呈现“大规模爆发”特征。
    * **BSP 特征**：Giraph 的网络流量（从 dstat 观察）呈现出明显的 **脉冲式（Pulse）** 特征。每一个脉冲对应一个 **Superstep** 的结束，此时所有节点进行全局同步（Synchronization Barrier）并交换消息。这验证了 BSP 模型中“计算-通信-同步”的周期性行为。

* **【调度对比】**
    * 调度策略对网络模式影响较小，但在 **Fair** 模式下，如果多个 Giraph 任务并行，可能会观察到网络带宽的竞争，导致同步屏障（Barrier）等待时间变长。

---

### 5. 任务调度与长尾指标 (Scheduling & Stragglers)
> **核心研究点**：分析负载均衡与 Straggler 对算法收敛速度的影响。

| 指标名称 | 英文对应 | 数据源 | 你的动作 | **监测目的与论文意义 (为什么测这个？)** |
| :--- | :--- | :--- | :--- | :--- |
| **任务时长标准差** | `Duration` (StdDev) | **[UI List]** | 导出 Task 时间计算标准差。 | **量化负载均衡 (Load Balance)**：标准差越大，说明任务分配越不均匀（数据倾斜）。在 Giraph 中，这意味着图切分算法（如 Hash Partitioning）效果不佳，导致某些 Worker 负载过重，这是导致整体性能下降的**根本原因**。 |
| **Straggler 数量** | (Avg * 1.5) | **[Script]** | 使用脚本统计拖后腿的任务数。 | **评估调度公平性**：验证 Fair/Capacity 调度器是否有效。Straggler 的存在会严重拖累 BSP 系统。如果优化调度策略后此数量减少，说明调度器成功改善了**资源分配的公平性**。 |
| **最慢 Task 耗时** | `Duration` (Max) | **[Hadoop UI]** | 找出列表中的最大值。 | **验证“木桶效应” (Bucket Effect)**：这是 BSP 模型最脆弱的一点。论文论据：**作业总耗时 $\approx$ (最慢 Task 耗时 × 迭代次数)**。此指标用于证明系统性能完全受限于那个“最慢的节点”，而非平均水平。 |
| **Map/Compute 进度** | UI Progress | **[Hadoop UI]** | 观察 99% 到 100% 卡了多久。 | **可视化同步开销 (Barrier Overheads)**：在 BSP 模型中，快任务必须等待慢任务到达同步点（Barrier）。进度条卡在 99% 的时间，就是**系统空转浪费的时间**。这是对比 Giraph (同步阻塞) 与异步图计算系统（如 GraphLab，虽然你可能不测，但可以提）时的重要理论依据。 |

**📝 分析：**

* **【框架对比：Giraph vs MR】**
    * **Straggler 敏感度**：Giraph 对 Straggler（长尾任务） **极其敏感**。由于 BSP 的同步机制，整个作业必须等待最慢的那个 Worker 完成当前的 Superstep 才能进入下一步（木桶效应）。
    * **MR 容错性**：MapReduce 对长尾任务的容忍度稍好，因为其 Map 和 Reduce 任务是相对独立的，且支持推测执行（Speculative Execution）来重跑慢任务。

* **【调度对比：FIFO vs Fair/Capacity】**
    * 在处理 **幂律分布（Power-law）** 的图数据时，Straggler 现象加剧。
    * **Fair 调度器** 表现出更好的资源分配粒度，在一定程度上缓解了因资源分配不均导致的等待。但在 FIFO 模式下，如果一个大任务占据了所有资源，后续任务完全无法启动，长尾效应被无限放大为作业级别的延迟。

### 6. 数据集操作速查 (Dataset Operations Checklist)
> **核心研究点**：通过不同规模的数据集，分别验证分布式系统的启动开销、并行调度优势以及架构的物理瓶颈。

| 数据集场景 | 关注重点 (Focus) | 数据源 | 你的动作 | **监测目的与论文意义 (为什么测这个？)** |
| :--- | :--- | :--- | :--- | :--- |
| **Small 数据集**<br>(`random100`) | System Setup Time | **[UI Overview]** | 观察 Setup（灰色条）时间与总时间的占比。 | **验证“冷启动”成本 (Cold Start)**：证明分布式系统存在高昂的启动门槛。在极小数据量下，资源申请、JVM 启动与 ZooKeeper 协调的系统开销 (Overhead) 远大于计算收益，解释了为何“杀鸡焉用牛刀”会导致性能倒挂。 |
| **Medium 数据集**<br>(`stanford`) | Straggler & Wait Time | **[Script + UI]** | 重点运行调度对比，记录排队时间与长尾数。 | **验证“并行计算与调度优势”**：这是体现分布式集群优势的最佳场景（Sweet Spot）。此处数据最适合分析 **调度策略**（如 Fair）如何通过资源隔离缓解长尾效应，以及多 Worker 并行如何提升整体吞吐量。 |
| **Large 数据集**<br>(`roadNet`) | HDFS Bytes Written | **[UI Counters]** | **截图** MR 的 GB 级写入 vs Giraph 的 MB 级写入。 | **验证“数据流与物理瓶颈”**：这是架构差异的极限压测。用于证明 MapReduce 输在 **I/O 瓶颈**（磁盘流机制导致吞吐上限），而 Giraph 输在 **分区策略**（图结构导致通信风暴或内存溢出）。 |

**📝 分析模板：**

* **【规模效应：启动开销 vs 计算收益】**
    * **小数据陷阱**：在 Small 数据集中，MapReduce 和 Giraph 的大部分时间都消耗在 `Setup` 和 `Cleanup` 阶段。实验数据显示，计算阶段仅占总耗时的 **[填入百分比]%**，有力地论证了分布式框架在处理微小负载时的 **系统级损耗 (System Overhead)** 问题。
    * **甜蜜点 (Sweet Spot)**：Medium 数据集展现了最佳的加速比。此时数据量足够掩盖启动开销，且未达到集群内存瓶颈，能够充分发挥 Giraph 内存计算与多节点并行的优势。

* **【极限瓶颈：I/O 阻塞 vs 拓扑敏感性】**
    * **MapReduce 的 I/O 墙**：在 Large 数据集下，MapReduce 的性能瓶颈完全被 **HDFS 写入量** 锁定。每轮迭代产生的 **[填入 GB 数]** 级磁盘写入，使其运行时间随数据量呈线性甚至超线性增长。
    * **Giraph 的拓扑敏感性**：Giraph 虽然消除了 I/O 瓶颈，但在处理 `roadNet` 这类非幂律分布（高直径、网格状）图时，集群模式下的 **边割 (Edge Cut)** 导致了巨大的跨节点通信开销。这证明了 BSP 模型对 **图分区策略 (Partitioning Strategy)** 的高度依赖性，即“不仅要算得快，还要分得好”。
---
