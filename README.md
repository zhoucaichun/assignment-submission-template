# 作业提交模板

```
.
├── code/                   # 所有实验代码
└── README.md               # 项目核心文档
```

## 研究目的
比较 Giraph 和 MapReduce 运行 PageRank 算法的差异

## 研究内容
1. 对比分析 Giraph 和 MapReduce 在执行 PageRank 算法等图迭代计算任务时的差异。
2. 深入理解 Giraph 所采用的 BSP (Bulk Synchronous Parallel) 设计理念及其在图计算中的优势。
3. 重点探讨两者在数据通信方式、任务调度机制及迭代开销等方面的不同，以及这些差异对算法性能与可扩展性的影响。
**增加的研究内容：**

## 实验
1. 分别基于Giraph和MapReduce实现PageRank算法，从编程模型角度分析两种系统在算法表达与数据流处理方式上的差异。
2. 在不同规模的图数据集上运行实验，记录作业执行时间、网络通信量以及内存占用等指标，对比分析Giraph与MapReduce在图迭代计算中的性能表现。

### 实验环境
#### 硬件配置
本次实验部署在分布式集群上，包含 **1 个主节点** 和 **3 个子节点**，满足节点数 (>=3) 的要求。
* **节点拓扑**：
    * **Master**: `ecnu01` (NameNode, ResourceManager)
    * **Slaves**: `ecnu02`, `ecnu03`, `ecnu04` (DataNode, NodeManager)
* **单节点配置**：
    * **CPU**: 4 核 (vCPU)
    * **内存**: 8 GiB
    * **操作系统**: Ubuntu 24.04 64位
    * **网络带宽**: 100 Mbps (峰值)
    * **区域**: 华东2 (上海) 可用区 B

#### 软件配置
* **操作系统**：Linux
* **JDK 版本**：Java 8 OpenJDK
* **Hadoop 版本**：2.7.3 (HDFS + YARN)
* **Giraph 版本**：1.3.0-SNAPSHOT (针对 Hadoop 2.7.3 重新编译，已解决 Protobuf 2.5.0 冲突)
* **Build Tool**：Maven 3.x

### 实验负载
本次实验使用 PageRank 算法作为核心工作负载，测试数据集分为两组：
1.  **逻辑验证数据集 (`init_graph.txt`)**
    * **规模**：小规模手动构建数据（约 10 个节点）。
    * **格式**：Tab 分隔文本 (TSV)，`NodeID [TAB] PR_Value [TAB] OutLink1,OutLink2...`。
    * **用途**：用于验证 MapReduce 链式作业的迭代逻辑正确性及收敛性。
2.  **性能测试数据集 (`input_json.txt`)**
    * **规模**：中等规模图数据。
    * **格式**：JSON 格式，`[顶点ID, 顶点值, [[目标顶点ID1, 边权重1], ...]]`。
    * **用途**：作为 Giraph 框架的标准输入，同时用于对比两种框架在大规模迭代下的运行时长。

### 实验步骤
列出执行实验的关键步骤，并对关键步骤进行截图，如 MapReduce / Spark / Flink 部署成功后的进程信息、作业执行成功的信息等，**截图能够通过显示用户账号等个性化信息佐证实验的真实性**。
#### 一、 环境与服务检查

在 Master 节点 (`ecnu01`) 检查 HDFS 和 YARN 服务状态，确保所有 Slave 节点正常在线。

- 命令：`jps`
- 预期：Master 节点包含 `ResourceManager`, `NameNode`；Slave 节点包含 `NodeManager`, `DataNode`。

*(此处待补充：Master 和 Slave 节点的 jps 运行截图)*

#### 二、 实验设计


#### 三、 MapReduce PageRank 实验

**1. 算法编译与打包**
在本地开发环境使用 Maven 编译 MapReduce 实现代码，生成 Jar 包。

```bash
mvn clean package
```

#### 四、 Giraph PageRank 实验

### 实验结果与分析

本节基于 **Capacity 调度器** 环境，对比了 MapReduce 与 Giraph 在处理相同数据集 (`stanford_input`) 时的性能表现。我们通过 `dstat` 采集了全过程的 CPU、内存、网络及磁盘指标。

##### 1. 核心指标对比汇总

下表展示了两次实验的关键性能指标对比。

| 监控维度 | 关键指标 | **Giraph (BSP模型)** | **MapReduce (传统模型)** | **对比结论** |
| --- | --- | --- | --- | --- |
| **作业效率** | **运行总耗时** | **42 秒** | **591 秒** | **Giraph 快 14 倍**，完全碾压 MapReduce。 |
| **资源消耗** | **CPU 利用率 (Avg)** | **44.9%** (User+Sys) | **37.1%** (User+Sys) | Giraph 计算密度更高，CPU 一直在有效工作；MR 存在等待间隙。 |
| **通信开销** | **网络吞吐峰值** | **44.0 MB/s** | **89.6 MB/s** | MR 的 Shuffle 阶段引发了比 Giraph 更猛烈的网络风暴。 |
| **内存占用** | **内存峰值** | **1.51 GB** | **1.54 GB** | 两者内存占用相当（受限于小数据规模，未触及内存瓶颈）。 |
| **I/O 特征** | **磁盘读写** | **~0 MB/s** | **~0 MB/s*** (见下方分析) | 在小数据下，两者物理磁盘 I/O 均不明显。 |

##### 2. 深入图表分析

###### (1) 网络通信模式对比：BSP 脉冲 vs Shuffle 风暴

- **Giraph (图A)**：网络流量呈现出清晰的 **“周期性脉冲”** 特征。每一个波峰对应一个 Superstep 的结束，验证了 BSP 模型中 **“计算 -> 同步(发消息) -> 计算”** 的执行节奏。
- **MapReduce (图B)**：网络流量呈现 **“集中爆发”** 特征（峰值高达 89.6 MB/s）。这是典型的 **Shuffle 阶段** 行为，所有节点同时在网络上拉取数据，极易造成网络拥塞。

###### (2) CPU 负载特征分析

- **Giraph**：CPU 曲线较为平稳且持续处于中高位（45%左右），说明计算节点一直在进行图的迭代计算，没有明显的阻塞。
- **MapReduce**：CPU 曲线存在波动。虽然平均利用率较低（37%），但这是因为 CPU 经常需要等待数据传输（Shuffle）或序列化操作，导致计算资源利用不充分。

###### (3) 关于“磁盘 I/O 为零”的特别说明

在本次实验中，我们观测到 MapReduce 的物理磁盘 I/O 极低。这**并非**意味着 MapReduce 不读写磁盘，而是因为：

- **Linux Page Cache 机制**：本次实验数据集仅 38MB，远小于节点内存（8GB）。操作系统自动将所有中间文件缓存在了 RAM 中，导致物理磁盘读写被“屏蔽”。
- **推论**：如果数据量增加到 10GB 以上（超过内存容量），MapReduce 的磁盘 I/O 曲线将会剧烈飙升，而 Giraph 将因内存溢出而失败或性能急剧下降。

### 实验结论

1. **性能差异**：在迭代图计算场景下，Giraph 凭借 **常驻内存 (In-Memory)** 和 **BSP 消息传递** 机制，比基于磁盘 Shuffle 的 MapReduce 快了一个数量级（14x）。
2. **调度影响**：在 Capacity 调度下，两者均能获得稳定的资源，但 MapReduce 对网络带宽的瞬时压力更大，更容易干扰集群中的其他作业。
3. **适用场景**：
    - **Giraph** 适合处理 **“存得下”** 的图数据，追求极致速度。
    - **MapReduce** 适合处理 **“超大规模”**（远超内存）的数据清洗和一次性计算任务，但不适合迭代算法。

### 结论

总结研究的主要发现。

### 分工

尽可能详细地写出每个人的具体工作和贡献度，并按贡献度大小进行排序。
